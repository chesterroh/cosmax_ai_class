{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba63d05-5997-49ee-adac-f7b710a46f7e",
   "metadata": {},
   "source": [
    "\n",
    "2022.03.17 오늘 수업에서 말씀드렸지만 transformer 를 이해하기 위해서는 RNN 을 이용한 seq2seq 에서 dataset 이 어떻게 만들어지고, forward propagation 을 지나가면서 어떻게 tensor dimension 이 변하는지를 한번 이해해두면 큰 도움이 됩니다. 그러니 조금 개념이 어렵더라도 RNN 을 이용한 seq2seq 도 적어도(?) 한번정도는 이해해두면 좋을 것같습니다. \n",
    "\n",
    "transformer 와 관련하여 엄청나게 많은 자료들이 있습니다만, 대부분의 동영상이나 글들이 seq2seq / seq2seq + attention 에서 사용되는 NLP 기법들에 대해서 거의 다 알고 있다는 가정을 하고 나서 진행되기 때문에 RNN 을 이용한 NLP 에 대한 대략적인 이해없이는 살짝 어렵게 느껴질 수 있습니다. seq2seq+attention 설명을 하면서 Decoder 의 전단계의 hidden 이 Query 역할을 하며, Encoder 의 hidden outputs 들이 Key 역할을 한다는 말씀을 드렸죠. Attention 에 대한 기본 개념에 대해서 어느정도 이해가 될 수 있었으면 좋겠습니다. \n",
    "\n",
    "## 허민석님의 동영상 강연이 transformer 상위개념을 잡기에 좋습니다.\n",
    "\n",
    "https://www.youtube.com/watch?v=mxGCEWOxfe8\n",
    "\n",
    "## transformer 의 내부구조와 tensor dimension 들의 변화를 가장 깔끔하게 설명하고 도식화한 자료는 이 녀석 같습니다.\n",
    "\n",
    "- Part-1 ( https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452 )\n",
    "- Part-2 ( https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34 )\n",
    "- Part-3 ( https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853 )\n",
    "- Part-4 ( https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3 ) \n",
    "\n",
    "저는 transformer 를 이해하면서 tensor dimension 과 masked attention 의 개념을 이해하기 위해서 Part-2/Part-3 글을 꽤나 여러번 반복해서 읽었습니다. \n",
    "\n",
    "시중에 유명하다고 하는 Jay Alammar 가 만든 The Illustrated Transformer 는 오히려 이해하 어렵게 도식화가 되어 있어서 저는 권하지 않습니다. 오히려 혼돈스럽기만 하더라구요. \n",
    "\n",
    "## Self-Attention 의 개념에 대해서 참고할만한 동영상은 이녀석입니다.\n",
    "\n",
    "https://www.youtube.com/watch?v=1BFE1Tfs8tM  ( 불행히도 이 친구의 다른 씨리즈는 너무 단순화를 많이 해둬서 공부하는게 좀 무의해보입니다. 그냥 self-attention 의 개념이 이런거구나~ 정도를 이해하시는데 좋습니다. )\n",
    "\n",
    "## Positional Encoding \n",
    "\n",
    "positional embedding 에 대한 개념도 처음에 참 받아들이기가 어렵습니다. Positional Encoding 을 이해하는데 가장 정리가 잘되어 있는 동영상은 https://www.youtube.com/watch?v=dichIcUZfOw 인것 같습니다... 두번째로는  https://www.youtube.com/watch?v=1biZfFLPRSY  이 동영상을 추천해 드립니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2471ae74-cb7c-4e71-9bf9-ae0e298cf563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
