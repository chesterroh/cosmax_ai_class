{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 코드를 이용해서 우리가 얻어야 하는 것들의 목표\n",
    "\n",
    "* RNN 의 구조에 대한 다시 한번의 이해\n",
    "* input, output data 들의 shape 에 대한 이해\n",
    "* RNN 안에 어떠한 parameter 들이 존재하는지에 대한 이해\n",
    "* self-supervised learning 에 대한 intuition 에 대해서 이해\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day', 'cailab prevails', ]\n",
    "\n",
    "# dictionary 형성을 위해서 \n",
    "chars = set(''.join(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = { char: ind for ind,char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0, ' ': 1, 'g': 2, 'w': 3, 'l': 4, 'y': 5, 'u': 6, 'p': 7, 'c': 8, 'v': 9, 'b': 10, 's': 11, 'o': 12, 'h': 13, 'r': 14, 'f': 15, 'e': 16, 'm': 17, 'd': 18, 'a': 19, 'n': 20}\n"
     ]
    }
   ],
   "source": [
    "print(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "14\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in text:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = len(max(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding 을 maxlen 으로 만들어 넣는다... padding 은 ' '\n",
    "\n",
    "for i in range(len(text)):\n",
    "    while len(text[i]) < maxlen:\n",
    "        text[i] += ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey how are you', 'good i am fine ', 'have a nice day', 'cailab prevails']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n",
      "Input Sequence: cailab prevail\n",
      "Target Sequence: ailab prevails\n"
     ]
    }
   ],
   "source": [
    "# input / target 을 sequence 를 generate\n",
    "\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    input_seq.append(text[i][:-1])\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [ char2int[c] for c in input_seq[i]]\n",
    "    target_seq[i] = [ char2int[c] for c in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 들을 one-hot representation 으로 바꿈 \n",
    "\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode( sequence, dict_size, seq_len, batch_size):\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype= np.float32)\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i,u, sequence[i][u] ] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 14, 21)\n"
     ]
    }
   ],
   "source": [
    "input_seq = one_hot_encode( input_seq, dict_size, seq_len, batch_size)\n",
    "print(input_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14, 21])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model( input_size=dict_size, output_size=dict_size, hidden_dim = 12, n_layers = 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "lr = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 / 1000\n",
      "2.6153151988983154\n",
      "20 / 1000\n",
      "2.3262863159179688\n",
      "30 / 1000\n",
      "1.9209972620010376\n",
      "40 / 1000\n",
      "1.5043874979019165\n",
      "50 / 1000\n",
      "1.126924753189087\n",
      "60 / 1000\n",
      "0.8069230914115906\n",
      "70 / 1000\n",
      "0.5786339044570923\n",
      "80 / 1000\n",
      "0.4218820035457611\n",
      "90 / 1000\n",
      "0.31221243739128113\n",
      "100 / 1000\n",
      "0.23614656925201416\n",
      "110 / 1000\n",
      "0.18467701971530914\n",
      "120 / 1000\n",
      "0.14975956082344055\n",
      "130 / 1000\n",
      "0.12546086311340332\n",
      "140 / 1000\n",
      "0.10801015049219131\n",
      "150 / 1000\n",
      "0.0950629711151123\n",
      "160 / 1000\n",
      "0.08515185117721558\n",
      "170 / 1000\n",
      "0.07735756784677505\n",
      "180 / 1000\n",
      "0.0766797661781311\n",
      "190 / 1000\n",
      "0.07191787660121918\n",
      "200 / 1000\n",
      "0.06582465022802353\n",
      "210 / 1000\n",
      "0.06024587154388428\n",
      "220 / 1000\n",
      "0.05674416199326515\n",
      "230 / 1000\n",
      "0.053714819252491\n",
      "240 / 1000\n",
      "0.051319483667612076\n",
      "250 / 1000\n",
      "0.04923576861619949\n",
      "260 / 1000\n",
      "0.04744499921798706\n",
      "270 / 1000\n",
      "0.04586787149310112\n",
      "280 / 1000\n",
      "0.04446592181921005\n",
      "290 / 1000\n",
      "0.043212298303842545\n",
      "300 / 1000\n",
      "0.042084310203790665\n",
      "310 / 1000\n",
      "0.04106471315026283\n",
      "320 / 1000\n",
      "0.040139347314834595\n",
      "330 / 1000\n",
      "0.03929639980196953\n",
      "340 / 1000\n",
      "0.03852593153715134\n",
      "350 / 1000\n",
      "0.037819601595401764\n",
      "360 / 1000\n",
      "0.037170205265283585\n",
      "370 / 1000\n",
      "0.03657151386141777\n",
      "380 / 1000\n",
      "0.03601827099919319\n",
      "390 / 1000\n",
      "0.035505689680576324\n",
      "400 / 1000\n",
      "0.0350298210978508\n",
      "410 / 1000\n",
      "0.03458709269762039\n",
      "420 / 1000\n",
      "0.034174371510744095\n",
      "430 / 1000\n",
      "0.03378891199827194\n",
      "440 / 1000\n",
      "0.033428240567445755\n",
      "450 / 1000\n",
      "0.033090200275182724\n",
      "460 / 1000\n",
      "0.03277292847633362\n",
      "470 / 1000\n",
      "0.032474588602781296\n",
      "480 / 1000\n",
      "0.03219370171427727\n",
      "490 / 1000\n",
      "0.031928885728120804\n",
      "500 / 1000\n",
      "0.03167886659502983\n",
      "510 / 1000\n",
      "0.031442511826753616\n",
      "520 / 1000\n",
      "0.031218865886330605\n",
      "530 / 1000\n",
      "0.031006881967186928\n",
      "540 / 1000\n",
      "0.030805841088294983\n",
      "550 / 1000\n",
      "0.03061494044959545\n",
      "560 / 1000\n",
      "0.030433444306254387\n",
      "570 / 1000\n",
      "0.030260758474469185\n",
      "580 / 1000\n",
      "0.03009628877043724\n",
      "590 / 1000\n",
      "0.029939498752355576\n",
      "600 / 1000\n",
      "0.02978990413248539\n",
      "610 / 1000\n",
      "0.02964702807366848\n",
      "620 / 1000\n",
      "0.029510507360100746\n",
      "630 / 1000\n",
      "0.029379919171333313\n",
      "640 / 1000\n",
      "0.029254892840981483\n",
      "650 / 1000\n",
      "0.02913517877459526\n",
      "660 / 1000\n",
      "0.029020389541983604\n",
      "670 / 1000\n",
      "0.028910277411341667\n",
      "680 / 1000\n",
      "0.028804585337638855\n",
      "690 / 1000\n",
      "0.02870306372642517\n",
      "700 / 1000\n",
      "0.028605515137314796\n",
      "710 / 1000\n",
      "0.02851167507469654\n",
      "720 / 1000\n",
      "0.02842138521373272\n",
      "730 / 1000\n",
      "0.028334444388747215\n",
      "740 / 1000\n",
      "0.028250694274902344\n",
      "750 / 1000\n",
      "0.028169989585876465\n",
      "760 / 1000\n",
      "0.02809215523302555\n",
      "770 / 1000\n",
      "0.028017031028866768\n",
      "780 / 1000\n",
      "0.027944546192884445\n",
      "790 / 1000\n",
      "0.027874529361724854\n",
      "800 / 1000\n",
      "0.027806876227259636\n",
      "810 / 1000\n",
      "0.02774149365723133\n",
      "820 / 1000\n",
      "0.027678245678544044\n",
      "830 / 1000\n",
      "0.027617046609520912\n",
      "840 / 1000\n",
      "0.02755783125758171\n",
      "850 / 1000\n",
      "0.027500491589307785\n",
      "860 / 1000\n",
      "0.027444925159215927\n",
      "870 / 1000\n",
      "0.0273911040276289\n",
      "880 / 1000\n",
      "0.027338910847902298\n",
      "890 / 1000\n",
      "0.027288297191262245\n",
      "900 / 1000\n",
      "0.027239203453063965\n",
      "910 / 1000\n",
      "0.027191560715436935\n",
      "920 / 1000\n",
      "0.027145305648446083\n",
      "930 / 1000\n",
      "0.02710038237273693\n",
      "940 / 1000\n",
      "0.027056729421019554\n",
      "950 / 1000\n",
      "0.027014320716261864\n",
      "960 / 1000\n",
      "0.02697313390672207\n",
      "970 / 1000\n",
      "0.026933038607239723\n",
      "980 / 1000\n",
      "0.02689405344426632\n",
      "990 / 1000\n",
      "0.026856113225221634\n",
      "1000 / 1000\n",
      "0.026819197461009026\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training \n",
    "\n",
    "input_seq = input_seq.to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(input_seq)\n",
    "    output = output.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    target_seq = target_seq.view(-1).long()\n",
    "    loss = criterion(output, target_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch,\"/\",n_epochs)\n",
    "        print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "\n",
    "    out, hidden = model(character)\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval()\n",
    "    start = start.lower()\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "        \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey how are you'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample(model, 15, 'hey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(21, 12, batch_first=True)\n",
      "  (fc): Linear(in_features=12, out_features=21, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 21])\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "torch.Size([21, 12])\n",
      "torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
