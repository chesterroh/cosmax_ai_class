{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 코드를 이용해서 우리가 얻어야 하는 것들의 목표\n",
    "\n",
    "* RNN 의 구조에 대한 다시 한번의 이해\n",
    "* input, output data 들의 shape 에 대한 이해\n",
    "* RNN 안에 어떠한 parameter 들이 존재하는지에 대한 이해\n",
    "* self-supervised learning 에 대한 intuition 에 대해서 이해\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n', 'i', 'p', 'g', 'w', 'l', 's', 'o', 'm', 'h', 'b', 'v', 'y', 'a', 'd', 'f', 'e', 'c', 'u', ' ', 'r'}\n",
      "{0: 'n', 1: 'i', 2: 'p', 3: 'g', 4: 'w', 5: 'l', 6: 's', 7: 'o', 8: 'm', 9: 'h', 10: 'b', 11: 'v', 12: 'y', 13: 'a', 14: 'd', 15: 'f', 16: 'e', 17: 'c', 18: 'u', 19: ' ', 20: 'r'}\n",
      "{'n': 0, 'i': 1, 'p': 2, 'g': 3, 'w': 4, 'l': 5, 's': 6, 'o': 7, 'm': 8, 'h': 9, 'b': 10, 'v': 11, 'y': 12, 'a': 13, 'd': 14, 'f': 15, 'e': 16, 'c': 17, 'u': 18, ' ': 19, 'r': 20}\n"
     ]
    }
   ],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day', 'cailab prevails', ]\n",
    "\n",
    "# dictionary 형성을 위해서 \n",
    "chars = set(''.join(text))\n",
    "print(chars)\n",
    "int2char = dict(enumerate(chars))\n",
    "print(int2char)\n",
    "char2int = { char: ind for ind,char in int2char.items()}\n",
    "print(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 0, 'i': 1, 'p': 2, 'g': 3, 'w': 4, 'l': 5, 's': 6, 'o': 7, 'm': 8, 'h': 9, 'b': 10, 'v': 11, 'y': 12, 'a': 13, 'd': 14, 'f': 15, 'e': 16, 'c': 17, 'u': 18, ' ': 19, 'r': 20}\n"
     ]
    }
   ],
   "source": [
    "print(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "14\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in text:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = len(max(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding 을 maxlen 으로 만들어 넣는다... padding 은 ' '\n",
    "\n",
    "for i in range(len(text)):\n",
    "    while len(text[i]) < maxlen:\n",
    "        text[i] += ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey how are you', 'good i am fine ', 'have a nice day', 'cailab prevails']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n",
      "Input Sequence: cailab prevail\n",
      "Target Sequence: ailab prevails\n"
     ]
    }
   ],
   "source": [
    "# input / target 을 sequence 를 generate\n",
    "\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    input_seq.append(text[i][:-1])\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [ char2int[c] for c in input_seq[i]]\n",
    "    target_seq[i] = [ char2int[c] for c in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 14, 21])\n",
      "torch.Size([56])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(input_seq.shape)\n",
    "print(target_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 들을 one-hot representation 으로 바꿈 \n",
    "\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode( sequence, dict_size, seq_len, batch_size):\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype= np.float32)\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i,u, sequence[i][u] ] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 14, 21)\n"
     ]
    }
   ],
   "source": [
    "input_seq = one_hot_encode( input_seq, dict_size, seq_len, batch_size)\n",
    "print(input_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14, 21])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model( input_size=dict_size, output_size=dict_size, hidden_dim = 12, n_layers = 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "lr = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 / 1000\n",
      "2.678593873977661\n",
      "20 / 1000\n",
      "2.40452241897583\n",
      "30 / 1000\n",
      "2.04549241065979\n",
      "40 / 1000\n",
      "1.6295890808105469\n",
      "50 / 1000\n",
      "1.2486501932144165\n",
      "60 / 1000\n",
      "0.9294658899307251\n",
      "70 / 1000\n",
      "0.6763091683387756\n",
      "80 / 1000\n",
      "0.4815254509449005\n",
      "90 / 1000\n",
      "0.33663490414619446\n",
      "100 / 1000\n",
      "0.2390192747116089\n",
      "110 / 1000\n",
      "0.1789623647928238\n",
      "120 / 1000\n",
      "0.14170360565185547\n",
      "130 / 1000\n",
      "0.11744683235883713\n",
      "140 / 1000\n",
      "0.10073792189359665\n",
      "150 / 1000\n",
      "0.0886378139257431\n",
      "160 / 1000\n",
      "0.07951264828443527\n",
      "170 / 1000\n",
      "0.07240192592144012\n",
      "180 / 1000\n",
      "0.0667174756526947\n",
      "190 / 1000\n",
      "0.0620814748108387\n",
      "200 / 1000\n",
      "0.05823933333158493\n",
      "210 / 1000\n",
      "0.05501231923699379\n",
      "220 / 1000\n",
      "0.052270714193582535\n",
      "230 / 1000\n",
      "0.049917690455913544\n",
      "240 / 1000\n",
      "0.04787976294755936\n",
      "250 / 1000\n",
      "0.04610017314553261\n",
      "260 / 1000\n",
      "0.044534582644701004\n",
      "270 / 1000\n",
      "0.04314780235290527\n",
      "280 / 1000\n",
      "0.04191156476736069\n",
      "290 / 1000\n",
      "0.04080302268266678\n",
      "300 / 1000\n",
      "0.03980341553688049\n",
      "310 / 1000\n",
      "0.03889733552932739\n",
      "320 / 1000\n",
      "0.03807225078344345\n",
      "330 / 1000\n",
      "0.037318140268325806\n",
      "340 / 1000\n",
      "0.03662734106183052\n",
      "350 / 1000\n",
      "0.03599413111805916\n",
      "360 / 1000\n",
      "0.03541361168026924\n",
      "370 / 1000\n",
      "0.034881021827459335\n",
      "380 / 1000\n",
      "0.0343916155397892\n",
      "390 / 1000\n",
      "0.033940818160772324\n",
      "400 / 1000\n",
      "0.033524420112371445\n",
      "410 / 1000\n",
      "0.03313879668712616\n",
      "420 / 1000\n",
      "0.03278077393770218\n",
      "430 / 1000\n",
      "0.03244758024811745\n",
      "440 / 1000\n",
      "0.032136838883161545\n",
      "450 / 1000\n",
      "0.03184637054800987\n",
      "460 / 1000\n",
      "0.031574420630931854\n",
      "470 / 1000\n",
      "0.0313192717730999\n",
      "480 / 1000\n",
      "0.031079521402716637\n",
      "490 / 1000\n",
      "0.030853822827339172\n",
      "500 / 1000\n",
      "0.03064104914665222\n",
      "510 / 1000\n",
      "0.030440209433436394\n",
      "520 / 1000\n",
      "0.030250290408730507\n",
      "530 / 1000\n",
      "0.030070533975958824\n",
      "540 / 1000\n",
      "0.029900159686803818\n",
      "550 / 1000\n",
      "0.029738498851656914\n",
      "560 / 1000\n",
      "0.029584918171167374\n",
      "570 / 1000\n",
      "0.029438849538564682\n",
      "580 / 1000\n",
      "0.0292997844517231\n",
      "590 / 1000\n",
      "0.029167283326387405\n",
      "600 / 1000\n",
      "0.02904089353978634\n",
      "610 / 1000\n",
      "0.028920216485857964\n",
      "620 / 1000\n",
      "0.028804916888475418\n",
      "630 / 1000\n",
      "0.02869466505944729\n",
      "640 / 1000\n",
      "0.02858910523355007\n",
      "650 / 1000\n",
      "0.028488028794527054\n",
      "660 / 1000\n",
      "0.028391137719154358\n",
      "670 / 1000\n",
      "0.02829817868769169\n",
      "680 / 1000\n",
      "0.028208959847688675\n",
      "690 / 1000\n",
      "0.02812325954437256\n",
      "700 / 1000\n",
      "0.02804088033735752\n",
      "710 / 1000\n",
      "0.027961665764451027\n",
      "720 / 1000\n",
      "0.027885396033525467\n",
      "730 / 1000\n",
      "0.027812005952000618\n",
      "740 / 1000\n",
      "0.027741286903619766\n",
      "750 / 1000\n",
      "0.02767307497560978\n",
      "760 / 1000\n",
      "0.02760731801390648\n",
      "770 / 1000\n",
      "0.02754385583102703\n",
      "780 / 1000\n",
      "0.027482585981488228\n",
      "790 / 1000\n",
      "0.027423398569226265\n",
      "800 / 1000\n",
      "0.027366170659661293\n",
      "810 / 1000\n",
      "0.027310889214277267\n",
      "820 / 1000\n",
      "0.027257392182946205\n",
      "830 / 1000\n",
      "0.027205636724829674\n",
      "840 / 1000\n",
      "0.027155522257089615\n",
      "850 / 1000\n",
      "0.027106998488307\n",
      "860 / 1000\n",
      "0.027059990912675858\n",
      "870 / 1000\n",
      "0.02701439894735813\n",
      "880 / 1000\n",
      "0.02697020210325718\n",
      "890 / 1000\n",
      "0.026927310973405838\n",
      "900 / 1000\n",
      "0.026885727420449257\n",
      "910 / 1000\n",
      "0.02684534154832363\n",
      "920 / 1000\n",
      "0.026806127279996872\n",
      "930 / 1000\n",
      "0.02676808461546898\n",
      "940 / 1000\n",
      "0.02673107571899891\n",
      "950 / 1000\n",
      "0.026695100590586662\n",
      "960 / 1000\n",
      "0.026660142466425896\n",
      "970 / 1000\n",
      "0.02662612684071064\n",
      "980 / 1000\n",
      "0.026593023911118507\n",
      "990 / 1000\n",
      "0.026560846716165543\n",
      "1000 / 1000\n",
      "0.02652951516211033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training \n",
    "\n",
    "input_seq = input_seq.to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(input_seq)\n",
    "    output = output.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    target_seq = target_seq.view(-1).long()\n",
    "    loss = criterion(output, target_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch,\"/\",n_epochs)\n",
    "        print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "\n",
    "    out, hidden = model(character)\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval()\n",
    "    start = start.lower()\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "        \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey how are you'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample(model, 15, 'he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(21, 12, batch_first=True)\n",
      "  (fc): Linear(in_features=12, out_features=21, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 21])\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "torch.Size([21, 12])\n",
      "torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
