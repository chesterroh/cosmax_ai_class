{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2x3 ( 2 rows, 3 columns )\n",
    "my_tensor = torch.tensor(\n",
    "    [ [1,2,3], [4,5,6]], dtype=torch.float32, device=device, requires_grad=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 cuda:0 torch.Size([2, 3]) True\n"
     ]
    }
   ],
   "source": [
    "print(my_tensor.dtype, my_tensor.device, my_tensor.shape, my_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 형태의 helper 를 이용한 tensor 정의 방법들\n",
    "\n",
    "x = torch.empty(size=(3,3))\n",
    "x = torch.zeros((3,3))   #  zeros(3,3) 이 아님을 유의\n",
    "x = torch.rand((3,3))   # 3x3 uniform distribution [0,1]\n",
    "x = torch.eye(5,5)\n",
    "x = torch.arange( start=0, end=5, step=0.1, dtype=torch.float64)\n",
    "x = torch.linspace(start=0.1, end=1, steps=10)\n",
    "x = torch.empty( size=(1,5)).normal_( mean=0, std=1 ) \n",
    "x = torch.empty( size=(1,5)).uniform_( 0, 1)\n",
    "x = torch.diag(torch.ones(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True])\n",
      "tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(4)\n",
    "print(t.bool())\n",
    "print(t.short()) #  dtype=torch.int16 으로 변환\n",
    "print(t.long())\n",
    "print(t.half())  #  원래의 float32 에서 float16 으로 변환\n",
    "print(t.float()) #   float32 가 default value\n",
    "print(t.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.zeros((5,5))\n",
    "t = torch.from_numpy(np_array)\n",
    "np_array_again = t.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([2., 4., 6.])\n",
      "tensor([[0.1519, 0.3852, 0.4337, 0.2215, 0.9684],\n",
      "        [0.5537, 0.0550, 0.0554, 0.8018, 0.8557],\n",
      "        [0.8642, 0.8678, 0.2414, 0.0062, 0.4056],\n",
      "        [0.2332, 0.5582, 0.7583, 0.7612, 0.2218],\n",
      "        [0.5649, 0.2614, 0.3992, 0.4100, 0.2449]])\n",
      "tensor([[2.0328, 1.9135, 1.8165, 2.0186, 2.6190],\n",
      "        [2.5110, 2.3084, 1.9562, 2.2081, 2.5899],\n",
      "        [2.3583, 2.0617, 2.1643, 2.3334, 2.5497],\n",
      "        [2.7150, 2.4004, 2.2546, 2.7059, 3.3015],\n",
      "        [2.0529, 1.7248, 1.5848, 1.8974, 2.2712]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor([1,2,3])\n",
    "y = torch.tensor([9,8,7])\n",
    "\n",
    "z1 = torch.empty(3)\n",
    "torch.add(x, y, out=z1)\n",
    "z2 = torch.add(x,y)\n",
    "z = x + y\n",
    "\n",
    "z = x - y   \n",
    "\n",
    "z = torch.true_divide(x, y)  # element-wise division \n",
    "\n",
    "t = torch.zeros(3)\n",
    "t.add_(x)   #   끝에 _ 를 붙히면 tensor 안에 있는 내용을 mutate 시킨다.\n",
    "print(t)\n",
    "t = t + x\n",
    "print(t)\n",
    "\n",
    "z = x.pow(2)\n",
    "z = x ** 2  # 똑같은 표현\n",
    "\n",
    "\n",
    "z = x > 0     # [True, True, True]\n",
    "z = x < 0     # [False, False, False]\n",
    "\n",
    "# matrix multiplication \n",
    "\n",
    "x1 = torch.rand((2,5))\n",
    "x2 = torch.rand((5,3))\n",
    "x3 = torch.mm( x1, x2) # \n",
    "x3 = x1.mm(x2)\n",
    "\n",
    "\n",
    "# matrix exponentiation \n",
    "\n",
    "matrix_exp = torch.rand(5,5)\n",
    "print(matrix_exp)\n",
    "print(matrix_exp.matrix_power(3))\n",
    "\n",
    "z = x * y    #    element-wise multiplication\n",
    "\n",
    "z = torch.dot(x,y)   # dot product \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 20]) torch.Size([32, 20, 30]) torch.Size([32, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "# batch matrix multiplication\n",
    "\n",
    "batch = 32\n",
    "n = 10\n",
    "m = 20\n",
    "p = 30\n",
    "\n",
    "t1 = torch.rand((batch, n, m))\n",
    "t2 = torch.rand((batch, m, p))\n",
    "out_bmm = torch.bmm(t1, t2)   #     ( b x n x p)\n",
    "\n",
    "print(t1.shape, t2.shape, out_bmm.shape)   # batch matrix product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5]) torch.Size([1, 5])\n",
      "tensor([[-0.2736, -0.2141, -0.9848, -0.7274, -0.7399],\n",
      "        [-0.6250, -0.6181, -0.8786, -0.6728, -0.2390],\n",
      "        [-0.8852, -0.0414, -0.3515, -0.5804, -0.0457],\n",
      "        [-0.8581, -0.4022, -0.1177, -0.6736, -0.7106],\n",
      "        [-0.1517, -0.7355, -0.8129, -0.7269, -0.7439]])\n",
      "tensor([[0.7264, 0.7859, 0.0152, 0.2726, 0.2601],\n",
      "        [0.3750, 0.3819, 0.1214, 0.3272, 0.7610],\n",
      "        [0.1148, 0.9586, 0.6485, 0.4196, 0.9543],\n",
      "        [0.1419, 0.5978, 0.8823, 0.3264, 0.2894],\n",
      "        [0.8483, 0.2645, 0.1871, 0.2731, 0.2561]])\n"
     ]
    }
   ],
   "source": [
    "# tensor broadcasting\n",
    "\n",
    "x1 = torch.rand((5,5))\n",
    "x2 = torch.ones((1,5))\n",
    "print(x1.shape, x2.shape)\n",
    "\n",
    "z = ( x1 - x2)\n",
    "print(z)\n",
    "z = ( x1 ** x2)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor([[1,2,3], [4,5,6]])\n",
    "\n",
    "sum_x = torch.sum( x, dim=0)   # dim=0 에서의 합 \n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "values, indicies = torch.max(x, dim=0)   #   x.max(dim=0)\n",
    "values, indicies = torch.min(x, dim=0)   #   x.min(dim=0)\n",
    "abs_x = torch.abs(x)\n",
    "\n",
    "\n",
    "z = torch.argmax(x, dim=0)\n",
    "z = torch.argmin(x, dim=0)\n",
    "mean_x = torch.mean(x.float(), dim=0)\n",
    "z = torch.eq(x, y)   # element-wise comparison \n",
    "sorted_y, indicies = torch.sort(y, dim=0, descending=False)\n",
    "\n",
    "z = torch.clamp(x, min=0)   # ReLU 처럼 동작함\n",
    "\n",
    "x = torch.tensor( [1,0,1,1,1], dtype=torch.bool)\n",
    "z = torch.any(x)   # True\n",
    "z = torch.all(x)   # False \n",
    "\n",
    "# dim=0 , dim=1 이 혼돈스러울 수 있음... 다음번에 multi-dimenstion dataset 을 다루면서 각각의 케이스에서의 behavior 를 익혀가는 것으로 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 25])\n",
      "torch.Size([25])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "features =  25\n",
    "x = torch.rand( (batch_size, features))\n",
    "\n",
    "#print(x)\n",
    "\n",
    "# numpy indexing 이랑 비슷해서 매우 혼돈스러움... 혼돈스러운 indexing 표현은 되도록이면 쓰지 않는 것을 권장.. \n",
    "print(x.shape)\n",
    "print(x[0].shape)\n",
    "print(x[:,0].shape) \n",
    "print(x[2, 0:10].shape)\n",
    "\n",
    "x[0,0] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 8])\n",
      "tensor([0.1997])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "indicies =  [ 0, 2, 8]\n",
    "print(x[indicies])\n",
    "\n",
    "x = torch.rand((3,5))\n",
    "rows = torch.tensor([1,0])\n",
    "cols = torch.tensor([4,0])\n",
    "print(x[rows, cols])    # tricky 한 표현 ... numpy array indexing 이랑 같음... 그런데 이런 표현은 웬만하면 쓰지 않는 것을 권장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 9])\n",
      "tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.arange(10)\n",
    "print(x[ (x<2) | (x>8)])\n",
    "print(x[ x.remainder(2) == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([ 0,  1,  2,  3,  4,  5, 12, 14, 16, 18])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print( torch.where( x > 5, x*2, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0,0,1,2,2,3,4]).unique()\n",
    "print(x)\n",
    "print(x.ndimension())\n",
    "print(x.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "tensor([0, 3, 6, 1, 4, 7, 2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.arange(9)\n",
    "x = x.view(3,3)\n",
    "x = x.reshape(3,3)    # reshape 은 memory 에 contiguous 하게 copy 하기 때문에 view 보다는 시간이 조금 더  걸림.. 그러나 그 다음부터 연산속도는 조금 더 빠름\n",
    "\n",
    "y = x.t()\n",
    "print(y.is_contiguous())     #  False 가 리턴됨 \n",
    "y = x.t().reshape(-1)\n",
    "print(y.is_contiguous())     #  True 가 리턴됨 \n",
    "\n",
    "print(y.contiguous().view(9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6084, 0.3850, 0.6542, 0.4312, 0.5375],\n",
      "        [0.9946, 0.2912, 0.7616, 0.0570, 0.2484]]) tensor([[0.1833, 0.0126, 0.4294, 0.2522, 0.0937],\n",
      "        [0.0299, 0.0784, 0.0827, 0.1017, 0.8215]])\n",
      "torch.Size([4, 5])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# dimenstion 에 따른 add operation 의 상이함에 대해서 이해해 보도록 하기\n",
    "\n",
    "x1 = torch.rand(2,5)\n",
    "x2 = torch.rand(2,5)\n",
    "print(x1, x2)\n",
    "\n",
    "print(torch.cat((x1,x2), dim=0).shape)    #   4x5      ( column-wide concatenation )\n",
    "print(torch.cat((x1,x2), dim=1).shape)    #   2x10     ( row-wide concatenation )\n",
    "\n",
    "# 이 부분 계속 혼돈스러운 것 알고 있음.... 일단 dim=0 은 column (세로)로  operation 하고, dim=1 은 row( 가로) 로 operation 하는 것으로 이해하고 넘어가자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6084, 0.3850, 0.6542, 0.4312, 0.5375, 0.9946, 0.2912, 0.7616, 0.0570,\n",
      "        0.2484])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "z = x1.view(-1)     #  하나의 row-vector 로 다 unroll 해버림 \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = 64\n",
    "x = torch.rand((batch,2,5))   # 64 batch 의 2x5 training set 이라고 가정해보고..\n",
    "z = x.view(batch,-1)  # batch 의 차원을 유지한채로 나머지를 다 unrolling 한다..  mninst dataset 의 28x28 을 fully-connected 에 넣어야 할때 바로 쓰게 된다.\n",
    "\n",
    "\n",
    "## 또 한가지의 유요한 기능은 permute 에 대해서....  이를테면 batch 는 그대로 둔채로 dataset 의 column 과 row 를 바꾸어야 할 경우에  이렇게 쓸수 있음..\n",
    "\n",
    "z = x.permute(0,2,1)  #  즉, dimension-1 에 있던 것을 dimension-2 로 바꾸라...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 5])\n",
      "torch.Size([64, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "z = torch.chunk(x, chunks=2, dim=1)\n",
    "print(z[0].shape)\n",
    "print(z[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7],\n",
      "        [8],\n",
      "        [9]])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# tensor 에 additional dimension 을 더하고 싶다면....\n",
    "\n",
    "x = torch.arange(10)   #  [10]\n",
    "print(x)\n",
    "\n",
    "print(x.unsqueeze(0))      # dimension-0 에 한 차원을 더 집어 넣는다. \n",
    "print(x.unsqueeze(1))      # dimension-1 에 한 차원을 더 집어 넣는다. \n",
    "\n",
    "# 앞으로 deep learning 프로젝트를 하는데, 사실 코드 자체는 얼마 안길고 쓰는 함수도 몇개 안된다..  다면 그 안을 타고 흐르는 이 tensor 들의 dimension 맞추기 게임이라는 것을 더 알게 된다 ㅠ.ㅠ   \n",
    "\n",
    "print(x.unsqueeze(0).shape)\n",
    "print(x.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
      "tensor([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])\n",
      "torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(10).unsqueeze(0)\n",
    "print(x)\n",
    "x = x.unsqueeze(1)\n",
    "print(x)    #  1x1x10 tensor 가 될것임\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8237a466b4f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#  차원을 하나 걷어내고 싶다면.... 걷어내고 싶은 dimension 에서 squeeze 를 불러주면 된다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "z = x.squeeze(dim=0)   #  차원을 하나 걷어내고 싶다면.... 걷어내고 싶은 dimension 에서 squeeze 를 불러주면 된다.    \n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning 이라는 것이 Tensor 가 신나게 Flow~~~ 하는 과정중에 dimension 맞추기 게임이라는 것을 알게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
